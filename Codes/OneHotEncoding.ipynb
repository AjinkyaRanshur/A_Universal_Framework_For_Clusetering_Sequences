{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "#Marks the starting period\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasta_content=\"D:\\Repo\\A_Universal_Framework_For_Clusetering_Sequences\\Datasets\\Sorted_Sequences\\Cpg_Dataset_Sequences_Sorted.fa\"\n",
    "fasta_content=\"D:\\Repo\\A_Universal_Framework_For_Clusetering_Sequences\\Datasets\\Sorted_Sequences\\AJR_Dataset_Sequences_100k_Sorted.fa\"\n",
    "\n",
    "# Preprocessing: Extracting and encoding sequences\n",
    "def parse_fasta(content):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    max_len=0\n",
    "    with open(content, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                label = line[-3:].strip()  # Assuming label is the last character of the line\n",
    "                # label = line[-2].strip()\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                if max_len < len(line.strip()):\n",
    "                    max_len=len(line.strip())\n",
    "\n",
    "                sequences.append(line.strip())\n",
    "    \n",
    "    return sequences, labels,max_len\n",
    "\n",
    "# Extract sequences from the file\n",
    "sequences,labels,max_len = parse_fasta(fasta_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(seq,max_len):\n",
    "    column_dim=max_len\n",
    "    row_dim=5\n",
    "    encoded_vector=np.zeros((row_dim,column_dim))\n",
    "    i=0\n",
    "    # Encoding sequences into numerical format (A=0, T=1, C=2, G=3)\n",
    "    char_to_int = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
    "    for i, chr in enumerate(seq):\n",
    "        if chr in char_to_int:  # Check if the character is valid\n",
    "            row_indx = char_to_int[chr]\n",
    "            encoded_vector[row_indx][i] = 1\n",
    "\n",
    "    return encoded_vector\n",
    "\n",
    "encoded_sequences=[]\n",
    "\n",
    "encoded_sequences = np.array([one_hot_encoding(seq, max_len) for seq in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences=encoded_sequences.reshape(len(encoded_sequences), -1)\n",
    "\n",
    "print(encoded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Add a mapping for labels\n",
    "# label_to_int = {'0': 0, '1': 1}  # Example mapping; update based on your labels\n",
    "label_to_int = {'0A':1,'0T': 2,'0G': 3,'0C': 4, '1A': 5,'1T': 6,'1G': 7,'1C': 8} \n",
    "numerical_labels = [label_to_int[label] for label in labels]  # Convert labels to numerical format\n",
    "# Create a TensorDataset with both sequences and labels\n",
    "padded_sequences_tensor = torch.tensor(encoded_sequences, dtype=torch.float32)\n",
    "numerical_labels_tensor = torch.tensor(numerical_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sequences and labels into a TensorDataset\n",
    "dataset = torch.utils.data.TensorDataset(padded_sequences_tensor, numerical_labels_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the Autoencoder to return both encoded and decoded representations\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(500, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 2), # Output 2 dimensions for latent space visualization\n",
    "            nn.ReLU()   \n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2,12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,500),\n",
    "            nn.ReLU()\n",
    "            # nn.Softmax(dim=1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded  # Return both encoded and decoded outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the model for latent space visualization\n",
    "input_dim = padded_sequences_tensor.shape[1]\n",
    "model = Autoencoder(input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch, _ in dataloader:  # Ignore labels during training\n",
    "        optimizer.zero_grad()\n",
    "        _, decoded = model(batch)\n",
    "        loss = criterion(decoded, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Visualization: Extract latent points\n",
    "model.eval()\n",
    "latent_points = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, labels in dataloader:\n",
    "        encoded, _ = model(batch)  # Extract encoded (latent) representations\n",
    "        latent_points.append(encoded.numpy())  # Convert latent points to NumPy\n",
    "        labels_list.append(labels.numpy())  # Convert labels to NumPy\n",
    "\n",
    "# Combine latent points and labels for plotting\n",
    "latent_points = np.concatenate(latent_points, axis=0)\n",
    "labels_list = np.concatenate(labels_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape after reshaping\n",
    "print(latent_points.shape)  # Should be (100000, 2)\n",
    "print(labels_list.shape)    # Should be (100000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA \n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# latent_points_2d = pca.fit_transform(latent_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Apply t-SNE on the latent points\n",
    "# tsne = TSNE(n_components=2, perplexity=20, max_iter=1000, random_state=42)\n",
    "# latent_points_2d = tsne.fit_transform(latent_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# # Apply UMAP to reduce to 2D\n",
    "# umap_reducer = umap.UMAP(n_components=2, n_neighbors=100, min_dist=0.1, random_state=42,n_jobs=-1)\n",
    "# latent_points_2d = umap_reducer.fit_transform(latent_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latent space\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(latent_points[:, 0], latent_points[:, 1], c=labels_list, cmap=\"tab10\", s=10)\n",
    "plt.colorbar(scatter, ticks=range(10), label=\"Digit Label\")\n",
    "plt.title(\"2D Latent Space Representation of CpG Island Sequences\")\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
